import pandas as pd
import numpy as np
from sklearn import preprocessing
from sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB
from sklearn.metrics import confusion_matrix, accuracy_score
from sklearn.model_selection import train_test_split
from matplotlib import pyplot as plt

print("\n\n********************************\n** Naive Bayes Classification **\n********************************\n")

#####import data and put names###
data = pd.read_csv("agaricus-lepiota.data", sep=",",  header=None)
data.columns = ["cls", "cap-shape", "cap-surface", "cap-color", "bruises",
                    "odor","gill-attachment", "gill-spacing","gill-size",
                    "gill-color","stalk-shape","stalk-root","stalk-surface-above-ring",
                    "stalk-surface-below-ring","stalk-color-above-ring","stalk-color-below-ring","veil-type",
                    "veil-color","ring-number","ring-type","spore-print-color",
                    "population","habitat"]


print("Variables of DataFrame data:")
print(data.columns)

# First replace in our dataframe string values '?' with a value that our libraries recognise as missing value.
# Note: '?' is for humans. NaN is for the pandas i.e. our libraries/machine wasy of saying missing values.  Human semantics are BAAAAAAD.
data = data.replace('?', np.NaN)

# We have now replaced '?' with NaN that means missing values.
# Now remove rows of our dataset that contain at least one missing value. This is done with dropna.
# Note: 0 means rows, how='any' means at least one NaN value is row.
data= data.dropna(0, how='any')

# Ok done. Missing values removed.
# Now we have to tell our dataset that the values 'y', 'n' are categorical (or factors in R parlance).
# This is called labelling in pandas. This will resule in one value 'y' to be encoded as 1 and 'n' as 0.
data = data.apply(preprocessing.LabelEncoder().fit_transform)

# We use the function train_test_split in sklearn to split the dataset into a training and testing set.
# 20% (0.2) of the dataset's observations is used as the testing set, while the rest is used as the
# training-set.
trainSet, testSet = train_test_split(data, test_size=0.2)

# Now, slice the trainSet to get the class attribute out (i.e. party membership) which is attribute at position
# 0 of the DataFrame
# trainClass will contain only the first column of the dataset (iloc enables integer-based indexing)
trainClass = trainSet.iloc[:,0] # -> all rows (:), first column (0). IMPORTANT: Python uses 0-based indexing!

# All other attributes except first one will form our training data
trainData = trainSet.iloc[:, 1:16]

# Slice the testing set into 2 parts:  one that contains only the first row which is the class attribute and
# another one that contains only all the other variables. We do this in order to calculate the confusion matrix
# at the end
testClass = testSet.iloc[:,0]
testData = testSet.iloc[:,1:16]

# Create the MultinomialNB Naive Bayes classifier object to properly use a probability
mNB = MultinomialNB()
NBModel = mNB.fit(trainData, trainClass)


#Predict the class attribute
testResults = NBModel.predict(testData)

# Done. Let's claculate and display the confusion matrix
cmatrix = confusion_matrix(testClass, testResults)
print(cmatrix)


# Let's see the accuracy of the Naive Bayes classifier we just built
accS = accuracy_score(testClass, testResults)
print("Accuracy score = ", accS )
